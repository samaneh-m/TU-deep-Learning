{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samaneh-m/TU-deep-Learning/blob/main/linear_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3abd0e2f",
      "metadata": {
        "id": "3abd0e2f"
      },
      "source": [
        "# Introduction to Fully Connected Layers\n",
        "\n",
        "In deep learning, a **fully connected layer** (or *linear layer*) is a fundamental building block of neural networks. It connects every neuron in the previous layer to every neuron in the current layer. This allows the network to learn complex relationships between features.\n",
        "\n",
        "### Input and Output Shapes\n",
        "\n",
        "Let the input tensor be denoted as:\n",
        "\n",
        "$X \\in \\mathbb{R}^{N \\times D_{\\text{in}}}$\n",
        "\n",
        "where:\n",
        "\n",
        "- $N$: batch size\n",
        "- $D_{\\text{in}}$: number of input features\n",
        "\n",
        "The fully connected layer applies a linear transformation to the input tensor, using a weight matrix $W$ and a bias vector $b$ with the following shapes:\n",
        "\n",
        "$W \\in \\mathbb{R}^{D_{\\text{in}} \\times D_{\\text{out}}}$\n",
        "\n",
        "$b \\in \\mathbb{R}^{D_{\\text{out}}}$\n",
        "\n",
        "where:\n",
        "\n",
        "- $D_{\\text{out}}$: number of output features\n",
        "\n",
        "The output tensor $Y$ has the shape:\n",
        "\n",
        "$Y \\in \\mathbb{R}^{N \\times D_{\\text{out}}}$\n",
        "\n",
        "### Linear Transformation\n",
        "\n",
        "Each output value $Y[n, d_{\\text{out}}]$ is computed as a weighted sum of the input features, plus a bias term:\n",
        "$$\n",
        "Y[n, d_{\\text{out}}] = \\left( \\sum_{d_{\\text{in}}=0}^{D_{\\text{in}}-1} X[n, d_{\\text{in}}] \\cdot W[d_{\\text{in}}, d_{\\text{out}}] \\right) + b[d_{\\text{out}}]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1654363f",
      "metadata": {
        "id": "1654363f"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03bd421f",
      "metadata": {
        "id": "03bd421f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from helper import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "881b11e7",
      "metadata": {
        "id": "881b11e7"
      },
      "source": [
        "## üõ† Implement the Forward Pass (2 Loops)\n",
        "\n",
        "üîπ***Task:*** Implement the forward pass of a fully connected layer by using **two nested loops**.\n",
        "\n",
        "üîç **Note:** You are only allowed to use these PyTorch functions for your code. This is all you need:\n",
        "- `torch.zeros`, `torch.sum`, `tensor.shape`\n",
        "output[i,j]=\n",
        "k\n",
        "‚àë\n",
        "‚Äã\n",
        " input[i,k]‚ãÖweight[j,k]+bias[j]\n",
        "\n",
        "Look into the [documentation](https://pytorch.org/docs/stable/torch.html) for a detailed function explanation. On the website, there is a searchbar at the top left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19e9a618",
      "metadata": {
        "id": "19e9a618",
        "outputId": "4a91bee7-4a51-4ef1-8e76-ebcc6f5cb963"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with in_features=4, out_features=8\n",
            "Testing with in_features=4, out_features=16\n",
            "Testing with in_features=8, out_features=8\n",
            "Testing with in_features=8, out_features=16\n",
            "Nice! Forward pass is correct. Move to the next task.\n"
          ]
        }
      ],
      "source": [
        "class MyLinearFunction_2_Loops(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias):\n",
        "        '''\n",
        "        Arguments:\n",
        "            input torch.Tensor -- input tensor of shape (N, D_in)\n",
        "            weight torch.Tensor -- weight tensor of shape (D_out, D_in)\n",
        "            bias torch.Tensor -- bias tensor of shape (D_out)\n",
        "\n",
        "        Returns:\n",
        "            output torch.Tensor -- output tensor of shape (N, D_out)\n",
        "        '''\n",
        "        ################################################\n",
        "        N, D_in = input.shape\n",
        "        D_out = weight.shape[0]\n",
        "        output = torch.zeros(N, D_out)  # initialize the output\n",
        "\n",
        "        for i in range(N):  # loop over each sample in the batch\n",
        "            for d_out in range(D_out):  # loop over each output feature\n",
        "                sum = 0\n",
        "                for d_in in range(D_in):  # loop over input features\n",
        "                    sum += input[i, d_in] * weight[d_out, d_in]\n",
        "                output[i, d_out] = sum + bias[d_out]  # add the bias\n",
        "\n",
        "        ################################################\n",
        "\n",
        "        # Save tensors and parameters for backward pass\n",
        "        ctx.save_for_backward(input, weight, bias)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return None\n",
        "\n",
        "class MyLinear_2_Loops(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Initializes the custom Linear layer.\n",
        "        \"\"\"\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return MyLinearFunction_2_Loops.apply(x, self.weight, self.bias)\n",
        "\n",
        "# test your implementation\n",
        "test_linear_forward(MyLinear_2_Loops)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d34814",
      "metadata": {
        "id": "58d34814"
      },
      "source": [
        "## üõ† Implement the Forward Pass (1 Loop)\n",
        "\n",
        "üîπ***Task:*** Implement the forward pass of a fully connected layer by using **one loop**. Loop over the batch size.\n",
        "\n",
        "üîç **Note:** You are only allowed to use these PyTorch functions for your code. This is all you need:\n",
        "- `torch.zeros`, `torch.sum`, `tensor.shape`\n",
        "\n",
        "Look into the [documentation](https://pytorch.org/docs/stable/torch.html) for a detailed function explanation. On the website, there is a searchbar at the top left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79d712e",
      "metadata": {
        "id": "e79d712e",
        "outputId": "dd675944-c13a-475d-f9da-e660d5789193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with in_features=4, out_features=8\n",
            "Testing with in_features=4, out_features=16\n",
            "Testing with in_features=8, out_features=8\n",
            "Testing with in_features=8, out_features=16\n",
            "Nice! Forward pass is correct. Move to the next task.\n"
          ]
        }
      ],
      "source": [
        "class MyLinearFunction_1_Loop(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias):\n",
        "        '''\n",
        "        Arguments:\n",
        "            input torch.Tensor -- input tensor of shape (N, D_in)\n",
        "            weight torch.Tensor -- weight tensor of shape (D_out, D_in)\n",
        "            bias torch.Tensor -- bias tensor of shape (D_out)\n",
        "\n",
        "        Returns:\n",
        "            output torch.Tensor -- output tensor of shape (N, D_out)\n",
        "        '''\n",
        "        ################################################\n",
        "        N, D_in = input.shape\n",
        "        D_out = weight.shape[0]\n",
        "        output = torch.zeros(N, D_out)  # initialize the output\n",
        "        for n in range(N):\n",
        "            output[n] = torch.sum(input[n].unsqueeze(1) * weight.T, dim=0) + bias\n",
        "        ################################################\n",
        "\n",
        "        # Save tensors and parameters for backward pass\n",
        "        ctx.save_for_backward(input, weight, bias)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return None\n",
        "\n",
        "class MyLinear_1_Loop(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Initializes the custom Linear layer.\n",
        "        \"\"\"\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return MyLinearFunction_1_Loop.apply(x, self.weight, self.bias)\n",
        "\n",
        "# test your implementation\n",
        "test_linear_forward(MyLinear_1_Loop)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "225e25ad",
      "metadata": {
        "id": "225e25ad"
      },
      "source": [
        "## üõ† Implement the Forward Pass (No Loops)\n",
        "\n",
        "üîπ***Task:*** Implement the forward pass of a fully connected layer by using **no loops**.\n",
        "\n",
        "üîç **Note:** You are only allowed to use these PyTorch functions for your code. This is all you need:\n",
        "- `torch.matmul` or `@`, `tensor.T`\n",
        "\n",
        "Look into the [documentation](https://pytorch.org/docs/stable/torch.html) for a detailed function explanation. On the website, there is a searchbar at the top left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741db343",
      "metadata": {
        "id": "741db343",
        "outputId": "e72aaf26-45d8-4fd0-87b8-3e572d497be4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with in_features=4, out_features=8\n",
            "Testing with in_features=4, out_features=16\n",
            "Testing with in_features=8, out_features=8\n",
            "Testing with in_features=8, out_features=16\n",
            "Nice! Forward pass is correct. Move to the next task.\n"
          ]
        }
      ],
      "source": [
        "class MyLinearFunction_No_Loops(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias):\n",
        "        '''\n",
        "        Arguments:\n",
        "            input torch.Tensor -- input tensor of shape (N, D_in)\n",
        "            weight torch.Tensor -- weight tensor of shape (D_out, D_in)\n",
        "            bias torch.Tensor -- bias tensor of shape (D_out)\n",
        "\n",
        "        Returns:\n",
        "            output torch.Tensor -- output tensor of shape (N, D_out)\n",
        "        '''\n",
        "        ################################################\n",
        "        N, D_in = input.shape\n",
        "        D_out = weight.shape[0]\n",
        "        output = torch.zeros(N, D_out)\n",
        "        # Initialize the output tensor with the correct shape\n",
        "\n",
        "        # Matrix multiplication: input @ weight.T ‚Üí (N, D_out)\n",
        "        output = input @ weight.T + bias  # Broadcasting handles the bias add\n",
        "        ################################################\n",
        "\n",
        "        # Save tensors and parameters for backward pass\n",
        "        ctx.save_for_backward(input, weight, bias)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return None\n",
        "\n",
        "class MyLinear_No_Loops(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Initializes the custom Linear layer.\n",
        "        \"\"\"\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return MyLinearFunction_No_Loops.apply(x, self.weight, self.bias)\n",
        "\n",
        "# test your implementation\n",
        "test_linear_forward(MyLinear_No_Loops)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}